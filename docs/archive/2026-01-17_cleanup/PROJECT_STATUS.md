# Projekt-Status: M10 Factuality Evaluation

**Stand:** 28. Dezember 2025

## âœ… Implementiert

### 1. M10-Evaluation-Infrastruktur

**Scripts:**
- âœ… `scripts/run_m10_factuality.py` - Haupt-Runner fÃ¼r alle M10-Runs
- âœ… `scripts/aggregate_m10_results.py` - Aggregiert Ergebnisse zu Summary-Matrix
- âœ… `scripts/tune_from_baseline.py` - Baseline-Analyse mit FP-Issue-Types
- âœ… `scripts/select_best_tuned_run.py` - WÃ¤hlt besten Tuned Run (robuste Gates: recall + specificity, MCC/Balanced Acc)
  - Persistiert Auswahl-Metadaten als JSON (`best_run_selection.json`)
  - Generiert Top-5 Tabelle fÃ¼r Transparenz (`best_run_top5.csv`)
  - Fallback-BegrÃ¼ndung explizit (Kandidatenmenge, Zielmetrik, Tie-Breaker)
  - Gate-Status niemals gelogen (nur âœ… wenn wirklich erfÃ¼llt)
- âœ… `scripts/run_m10_complete.sh` - Kompletter Workflow-Script

**Config:**
- âœ… `configs/m10_factuality_runs.yaml` - Alle 6+ Runs definiert (Baseline, Tuned, Ablation, Tuning-Varianten)

**Dokumentation:**
- âœ… `M10_EVALUATION.md` - Hauptdokumentation
- âœ… `M10_TUNING_WORKFLOW.md` - Tuning-Workflow
- âœ… `M10_IMPLEMENTATION_SUMMARY.md` - Implementierungs-Ãœbersicht
- âœ… `QUICKSTART_M10.md` - Schnellstart
- âœ… `results/README.md` - Results-Struktur

### 2. Tuning-Strategien implementiert

**Erweiterte Tuning-Parameter:**
- âœ… `severity_min` - Filtert nach Issue-Severity (low/medium/high)
- âœ… `ignore_issue_types` - Ignoriert "noisy" Issue Types
- âœ… `uncertainty_policy` - 3 Modi:
  - `count_as_error` - Uncertain zÃ¤hlt wie incorrect
  - `non_error` - Uncertain zÃ¤hlt NICHT
  - `weight_0.5` - Uncertain zÃ¤hlt als 0.5 Issue-Punkte
- âœ… `decision_mode` - issues/score/either/both

**Evidence-Gate (zentrale FP-Reduktion):**
- âœ… **ClaimVerifier:** "incorrect" nur wenn `evidence_found==True` UND klarer Widerspruch
- âœ… **Safety-Downgrade:** Falls `verdict=="incorrect"` aber `evidence_found==False` â†’ downgrade zu "uncertain"
- âœ… **Strukturierte Evidence-Felder:** `evidence_spans_structured`, `evidence_quote`, `rationale`
- âœ… **Ziel:** Reduziert False Positives signifikant, da "incorrect" nur noch mit belegter Evidenz markiert wird

**Ablation-Studien:**
- âœ… `use_claim_extraction` - Claim-Extraktion deaktivierbar
- âœ… `use_claim_verification` - Claim-Verifikation deaktivierbar
- âœ… `use_spans` - Issue-Spans deaktivierbar

### 3. Automatische Dokumentation

- âœ… Pro Run: Metrics JSON + Examples JSONL + Markdown-Doku
- âœ… Summary-Matrix (CSV) mit allen Metriken
- âœ… Summary-MD mit Interpretation
- âœ… Reproduzierbarkeit: Commit Hash, Timestamp, Config-Dump

### 4. Results-Verzeichnis aufgerÃ¤umt

- âœ… Alte Test-Runs archiviert (`results/archive/pre_m10_runs/`)
- âœ… Struktur klar: `results/evaluation/runs/` fÃ¼r M10-Runs
- âœ… Cache-Dateien organisiert

## ðŸ“Š Aktuelle Ergebnisse

### Abgeschlossene Runs

**FRANK (Dev/Calibration):**
1. âœ… `factuality_frank_baseline_v1` - Baseline (abgeschlossen)
2. âœ… `factuality_frank_tuned_v1` - Tuned (abgeschlossen)
3. âœ… `factuality_frank_ablation_v1` - Ablation (abgeschlossen)
4. âœ… `factuality_frank_tune_severity_v1` - severity_min=medium (abgeschlossen)
5. âœ… `factuality_frank_tune_ignore_types_v1` - ignore_issue_types (abgeschlossen)
6. âœ… `factuality_frank_tune_uncertain_policy_v1` - uncertainty_policy=non_error (abgeschlossen)

**FineSumFact (Test):**
7. âœ… `factuality_finesumfact_final_v1` - Final (abgeschlossen)
8. âœ… `factuality_finesumfact_ablation_v1` - Ablation (abgeschlossen)

**Combined:**
9. âœ… `factuality_combined_final_v1` - Combined Final (abgeschlossen)

### Beste Ergebnisse (FRANK)

**Baseline:**
- Balanced Accuracy: 0.508
- Recall: 0.958
- Specificity: 0.057 (sehr niedrig!)

**Tuned (aktuell selektiert, aber problematisch):**
- Balanced Accuracy: 0.508
- Recall: 0.958
- Specificity: 0.057 (katastrophal niedrig!)
- **Problem:** Praktisch "alles ist ein Fehler" - Specificity von 0.057 bedeutet, dass nur 5.7% der korrekten Summaries als korrekt erkannt werden.

**Warum der alte Best-Run unsinnig war:**
- `factuality_frank_tuned_v1` wurde basierend nur auf Balanced Accuracy + Recall-Constraint ausgewÃ¤hlt
- Kein Specificity-Gate â†’ Run mit extrem niedriger Specificity (0.057) wurde gewÃ¤hlt
- Folge: System markiert fast alle Summaries als fehlerhaft, auch korrekte

**Neue Auswahlregel (implementiert):**
- Gate 1: `recall >= 0.90` (wie bisher)
- Gate 2: `specificity >= 0.20` (NEU - verhindert katastrophal niedrige Specificity)
- Optimierungsziel: `mcc` (Matthews Correlation Coefficient) ODER `balanced_accuracy` (wÃ¤hlbar)
- Tie-breaker: precision, dann f1

**Tuning-Varianten (alte):**
- `severity_min=medium`: Balanced Acc 0.489, Recall 0.064 (zu niedrig!)
- `ignore_types`: Balanced Acc 0.484, Recall 0.053 (zu niedrig!)
- `uncertainty_policy=non_error` (mit severity_min=medium): Balanced Acc 0.475, Recall 0.064 (zu niedrig!)

**Neue gezielte Tuning-Runs (Gruppe A/B/C):**
- Gruppe A: `uncertainty_policy=non_error`, `severity_min=low`, `error_threshold=1`
- Gruppe B: `uncertainty_policy=weight_0.5`, `severity_min=low`, `error_threshold=1`
- Gruppe C: `uncertainty_policy=count_as_error`, `severity_min=low`, `error_threshold=2`

### FineSumFact (Test-Set)

**Final:**
- Balanced Accuracy: 0.523
- Recall: 1.0
- Specificity: 0.046 (sehr niedrig!)

**Ablation:**
- Balanced Accuracy: 0.505
- Recall: 1.0
- Specificity: 0.009 (extrem niedrig!)

## ðŸ”„ NÃ¤chste Schritte

### 1. Tuning-Strategien anpassen

**Problem identifiziert:**
- `severity_min=medium` filtert zu aggressiv â†’ Recall bricht ein
- Viele False Positives haben "low" severity Issues
- Uncertainty-Policy allein hilft nicht genug

**MÃ¶gliche LÃ¶sungen:**
- âœ… Kombination: `severity_min=low` + `uncertainty_policy=non_error`
- âœ… Kombination: `severity_min=low` + selektive `ignore_issue_types`
- âœ… `uncertainty_policy=weight_0.5` testen
- âœ… Score-basierte Entscheidungen (wenn AUROC > 0.55)

### 2. Weitere Tuning-Runs

**âœ… Implementiert (Gruppe A/B/C):**
```yaml
- factuality_frank_tune_simple_non_error_v1
  severity_min: "low"
  uncertainty_policy: "non_error"
  error_threshold: 1
  
- factuality_frank_tune_simple_weight05_v1
  severity_min: "low"
  uncertainty_policy: "weight_0.5"
  error_threshold: 1
  
- factuality_frank_tune_simple_threshold2_v1
  severity_min: "low"
  uncertainty_policy: "count_as_error"
  error_threshold: 2
```

### 3. Best Run auswÃ¤hlen

```bash
python3 scripts/select_best_tuned_run.py --recall-min 0.90 --specificity-min 0.20 --target mcc
```

**Neue Kriterien (robust):**
- Gate 1: `recall >= 0.90` (Constraint)
- Gate 2: `specificity >= 0.20` (NEU - verhindert katastrophal niedrige Specificity)
- Optimierungsziel: `mcc` (Matthews Correlation Coefficient) ODER `balanced_accuracy` (wÃ¤hlbar)
- Tie-breaker: precision, dann f1

**Output:**
- Confusion Matrix (TP/TN/FP/FN)
- BegrÃ¼ndung warum der Run gewÃ¤hlt wurde
- YAML-Snippet fÃ¼r FineSumFact final (inkl. `decision_threshold_float`, `severity_weights` falls vorhanden)

### 4. FineSumFact Final anpassen

Nach Auswahl des besten FRANK-Runs:
- Config `factuality_finesumfact_final_v1` mit best-tuned Config aktualisieren
- FineSumFact Final erneut ausfÃ¼hren (falls nÃ¶tig)

## ðŸ“ Projekt-Struktur

```
veri-api/
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ m10_factuality_runs.yaml      # Alle Run-Configs
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ run_m10_factuality.py         # Haupt-Runner
â”‚   â”œâ”€â”€ aggregate_m10_results.py      # Aggregator
â”‚   â”œâ”€â”€ tune_from_baseline.py         # Baseline-Analyse
â”‚   â”œâ”€â”€ select_best_tuned_run.py      # Best-Run-Auswahl
â”‚   â””â”€â”€ run_m10_complete.sh           # Kompletter Workflow
â”œâ”€â”€ results/
â”‚   â”œâ”€â”€ evaluation/                   # Aktuelle M10-Evaluation
â”‚   â”‚   â”œâ”€â”€ runs/                     # Run-Management
â”‚   â”‚   â”œâ”€â”€ summary_matrix.csv        # Aggregierte Metriken
â”‚   â”‚   â””â”€â”€ summary.md                # Summary-Dokumentation
â”‚   â””â”€â”€ archive/                      # Archivierte Daten
â””â”€â”€ docs/                             # Dokumentation
```

## ðŸŽ¯ Akzeptanzkriterien Status

- âœ… M10-Infrastruktur vollstÃ¤ndig implementiert
- âœ… Tuning-Strategien implementiert (severity, ignore_types, uncertainty_policy)
- âœ… Automatische Dokumentation pro Run
- âœ… Summary-Matrix mit Balanced Accuracy
- âœ… FineSumFact als reines Test-Set (keine ParameterÃ¤nderungen nach FRANK)
- âœ… **Robuste Best-Run-Auswahl implementiert** - Specificity-Gate verhindert katastrophal niedrige Specificity
- âœ… **Neue gezielte Tuning-Runs** - Gruppe A/B/C (einfache Parameter-Kombinationen)
- ðŸ”„ **Evaluation lÃ¤uft** - Neue Runs werden ausgefÃ¼hrt und aggregiert

## ðŸ“ Wichtige Hinweise

1. **FRANK = Dev/Calibration** - Hier wird getuned
2. **FineSumFact = Test-Set** - Keine ParameterÃ¤nderungen nach FRANK!
3. **Optimierungsmetrik:** Balanced Accuracy (wegen unbalanced Klassen)
4. **Constraint:** Recall >= 0.90 (nicht komplett implodieren)
5. **Problem:** Specificity sehr niedrig (viele False Positives)

## ðŸš€ Quick Start

```bash
# 1. Baseline-Analyse
python3 scripts/tune_from_baseline.py

# 2. Neue Tuning-Runs ausfÃ¼hren
python3 scripts/run_m10_factuality.py configs/m10_factuality_runs.yaml --skip-baseline

# 3. Aggregation
python3 scripts/aggregate_m10_results.py

# 4. Best Run auswÃ¤hlen
python3 scripts/select_best_tuned_run.py
```

