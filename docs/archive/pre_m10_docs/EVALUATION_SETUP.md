# Evaluation Setup - M10

## âœ… Abgeschlossen: Projektstruktur aufgerÃ¤umt

### 1. Archivierung alter Evaluation-Runs
- Alle alten Runs wurden nach `results/archive/pre_m10_evaluation/` verschoben
- EnthÃ¤lt: frank/, finesumfact/, sumeval/, other/, tables/
- README dokumentiert den Inhalt

### 2. Projektstruktur bereinigt
- Doppelte `dashboard/` Verzeichnis entfernt (nur `app/dashboard/` bleibt)
- Veraltete Skripte archiviert (`eval_factuality_binary.py` â†’ `archive_eval_factuality_binary_v1.py`)

### 3. Neue Evaluationsstruktur erstellt

#### Verzeichnisse
```
results/
â”œâ”€â”€ archive/                    # Alte Runs (vor M10)
â”‚   â””â”€â”€ pre_m10_evaluation/
â””â”€â”€ evaluation/                 # Neue strukturierte Evaluation
    â”œâ”€â”€ factuality/
    â”œâ”€â”€ coherence/
    â”œâ”€â”€ readability/
    â””â”€â”€ explainability/

evaluation_configs/             # Run-Konfigurationen
â”œâ”€â”€ factuality_frank_test.json
â”œâ”€â”€ coherence_sumeval_test.json
â”œâ”€â”€ readability_sumeval_test.json
â””â”€â”€ explainability_full_test.json
```

#### Neues einheitliches Evaluationsskript
- `scripts/eval_unified.py`: UnterstÃ¼tzt alle Dimensionen
  - Factuality (binÃ¤r)
  - Coherence (kontinuierlich)
  - Readability (kontinuierlich)
  - Explainability (vollstÃ¤ndiges System)

## ğŸš€ NÃ¤chste Schritte: Evaluation durchfÃ¼hren

### 1. Einzelne Dimensionen evaluieren

#### Factuality (FRANK)
```bash
python scripts/eval_unified.py evaluation_configs/factuality_frank_test.json
```

#### Coherence (SummEval)
```bash
python scripts/eval_unified.py evaluation_configs/coherence_sumeval_test.json
```

#### Readability (SummEval)
```bash
python scripts/eval_unified.py evaluation_configs/readability_sumeval_test.json
```

### 2. VollstÃ¤ndiges System mit Explainability evaluieren

```bash
python scripts/eval_unified.py evaluation_configs/explainability_full_test.json
```

### 3. Run-Configs anpassen

Die Config-Dateien in `evaluation_configs/` kÃ¶nnen angepasst werden:
- `max_examples`: Anzahl Beispiele (null = alle)
- `llm_model`: Modell (z.B. "gpt-4o-mini")
- `thresholds`: Entscheidungskriterien
- `cache_enabled`: Caching aktivieren/deaktivieren

## ğŸ“Š Ergebnisse

Ergebnisse werden gespeichert in:
- `results/evaluation/<dimension>/run_<run_id>_<timestamp>.json` - Run-Summary mit Metriken
- `results/evaluation/<dimension>/predictions_<run_id>_<timestamp>.jsonl` - Per-Example Predictions
- `results/evaluation/<dimension>/cache_<model>_<version>.jsonl` - LLM-Cache (optional)

## ğŸ“ Notizen

- Alle Runs sind reproduzierbar durch Run-Configs
- Cache-Dateien ermÃ¶glichen schnelle Wiederholungen ohne neue LLM-Calls
- Legacy-Skripte bleiben verfÃ¼gbar fÃ¼r spezielle Use Cases

