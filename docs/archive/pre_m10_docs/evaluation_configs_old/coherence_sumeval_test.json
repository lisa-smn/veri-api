{
  "run_id": "coherence_sumeval_test_v1",
  "dataset": "sumeval",
  "split": "test",
  "dimension": "coherence",
  "llm_model": "gpt-4o-mini",
  "llm_temperature": 0.0,
  "llm_seed": 42,
  "prompt_versions": {
    "coherence": "v1"
  },
  "explainability_version": "m9_v1",
  "thresholds": {
    "gt_min": 1.0,
    "gt_max": 5.0
  },
  "max_examples": null,
  "cache_enabled": true,
  "description": "Coherence Evaluation auf SummEval Test-Set"
}

